2022-04-28T09:23:18,996 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-04-28T09:23:18,996 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-04-28T09:23:19,096 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.3
TS Home: /opt/conda/lib/python3.6/site-packages
Current directory: /root/tutorial-p-v9wplzjbdxlq/torchserve-demo
Temp directory: /tmp
Number of GPUs: 0
Number of CPUs: 2
Max heap size: 794 M
Python executable: /opt/conda/bin/python3.6
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /root/tutorial-p-v9wplzjbdxlq/torchserve-demo/model_store
Initial Models: my_tc=BERTSeqClassification.mar
Log dir: /root/tutorial-p-v9wplzjbdxlq/torchserve-demo/logs
Metrics dir: /root/tutorial-p-v9wplzjbdxlq/torchserve-demo/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 2
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /root/tutorial-p-v9wplzjbdxlq/torchserve-demo/model_store
Model config: N/A
2022-04-28T09:23:19,096 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.3
TS Home: /opt/conda/lib/python3.6/site-packages
Current directory: /root/tutorial-p-v9wplzjbdxlq/torchserve-demo
Temp directory: /tmp
Number of GPUs: 0
Number of CPUs: 2
Max heap size: 794 M
Python executable: /opt/conda/bin/python3.6
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /root/tutorial-p-v9wplzjbdxlq/torchserve-demo/model_store
Initial Models: my_tc=BERTSeqClassification.mar
Log dir: /root/tutorial-p-v9wplzjbdxlq/torchserve-demo/logs
Metrics dir: /root/tutorial-p-v9wplzjbdxlq/torchserve-demo/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 2
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /root/tutorial-p-v9wplzjbdxlq/torchserve-demo/model_store
Model config: N/A
2022-04-28T09:23:19,104 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-04-28T09:23:19,104 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-04-28T09:23:19,133 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: BERTSeqClassification.mar
2022-04-28T09:23:19,133 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: BERTSeqClassification.mar
2022-04-28T09:23:20,156 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model my_tc
2022-04-28T09:23:20,156 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model my_tc
2022-04-28T09:23:20,156 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model my_tc
2022-04-28T09:23:20,156 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model my_tc
2022-04-28T09:23:20,157 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model my_tc loaded.
2022-04-28T09:23:20,157 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model my_tc loaded.
2022-04-28T09:23:20,157 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: my_tc, count: 2
2022-04-28T09:23:20,157 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: my_tc, count: 2
2022-04-28T09:23:20,177 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-04-28T09:23:20,177 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-04-28T09:23:20,189 [DEBUG] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3.6, /opt/conda/lib/python3.6/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-04-28T09:23:20,189 [DEBUG] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3.6, /opt/conda/lib/python3.6/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-04-28T09:23:20,190 [DEBUG] W-9001-my_tc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3.6, /opt/conda/lib/python3.6/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001]
2022-04-28T09:23:20,190 [DEBUG] W-9001-my_tc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3.6, /opt/conda/lib/python3.6/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001]
2022-04-28T09:23:20,517 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-04-28T09:23:20,517 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-04-28T09:23:20,517 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-04-28T09:23:20,517 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-04-28T09:23:20,526 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-04-28T09:23:20,526 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-04-28T09:23:20,527 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-04-28T09:23:20,527 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-04-28T09:23:20,529 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-04-28T09:23:20,529 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-04-28T09:23:21,152 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-04-28T09:23:21,152 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-04-28T09:23:21,378 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137801
2022-04-28T09:23:21,381 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:26.384445190429688|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137801
2022-04-28T09:23:21,382 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:0.6155548095703125|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137801
2022-04-28T09:23:21,383 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:2.3|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137801
2022-04-28T09:23:21,384 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:1398.01953125|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137801
2022-04-28T09:23:21,385 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:2274.80859375|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137801
2022-04-28T09:23:21,386 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:64.0|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137801
2022-04-28T09:23:21,580 [INFO ] W-9001-my_tc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2022-04-28T09:23:21,582 [INFO ] W-9001-my_tc_1.0-stdout MODEL_LOG - [PID]6813
2022-04-28T09:23:21,582 [INFO ] W-9001-my_tc_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-28T09:23:21,583 [INFO ] W-9001-my_tc_1.0-stdout MODEL_LOG - Python runtime: 3.6.13
2022-04-28T09:23:21,583 [DEBUG] W-9001-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-my_tc_1.0 State change null -> WORKER_STARTED
2022-04-28T09:23:21,583 [DEBUG] W-9001-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-my_tc_1.0 State change null -> WORKER_STARTED
2022-04-28T09:23:21,589 [INFO ] W-9001-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2022-04-28T09:23:21,589 [INFO ] W-9001-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2022-04-28T09:23:21,609 [INFO ] W-9001-my_tc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2022-04-28T09:23:21,625 [INFO ] W-9001-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1651137801625
2022-04-28T09:23:21,625 [INFO ] W-9001-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1651137801625
2022-04-28T09:23:21,695 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-04-28T09:23:21,703 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - [PID]6812
2022-04-28T09:23:21,704 [DEBUG] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_tc_1.0 State change null -> WORKER_STARTED
2022-04-28T09:23:21,704 [DEBUG] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_tc_1.0 State change null -> WORKER_STARTED
2022-04-28T09:23:21,704 [INFO ] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-04-28T09:23:21,704 [INFO ] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-04-28T09:23:21,705 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-28T09:23:21,705 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - Python runtime: 3.6.13
2022-04-28T09:23:21,741 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-04-28T09:23:21,741 [INFO ] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1651137801741
2022-04-28T09:23:21,741 [INFO ] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1651137801741
2022-04-28T09:23:21,765 [INFO ] W-9001-my_tc_1.0-stdout MODEL_LOG - model_name: my_tc, batchSize: 1
2022-04-28T09:23:21,821 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - model_name: my_tc, batchSize: 1
2022-04-28T09:23:22,534 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - Transformers version 4.18.0
2022-04-28T09:23:22,583 [INFO ] W-9001-my_tc_1.0-stdout MODEL_LOG - Transformers version 4.18.0
2022-04-28T09:23:24,883 [WARN ] W-9000-my_tc_1.0-stderr MODEL_LOG - Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.dense.weight']
2022-04-28T09:23:24,885 [WARN ] W-9000-my_tc_1.0-stderr MODEL_LOG - - This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
2022-04-28T09:23:24,886 [WARN ] W-9000-my_tc_1.0-stderr MODEL_LOG - - This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2022-04-28T09:23:24,887 [WARN ] W-9000-my_tc_1.0-stderr MODEL_LOG - Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']
2022-04-28T09:23:24,890 [WARN ] W-9000-my_tc_1.0-stderr MODEL_LOG - You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2022-04-28T09:23:24,934 [WARN ] W-9001-my_tc_1.0-stderr MODEL_LOG - Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
2022-04-28T09:23:24,935 [WARN ] W-9001-my_tc_1.0-stderr MODEL_LOG - - This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
2022-04-28T09:23:24,936 [WARN ] W-9001-my_tc_1.0-stderr MODEL_LOG - - This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2022-04-28T09:23:24,939 [WARN ] W-9001-my_tc_1.0-stderr MODEL_LOG - Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']
2022-04-28T09:23:24,940 [WARN ] W-9001-my_tc_1.0-stderr MODEL_LOG - You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2022-04-28T09:23:24,954 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - <All keys matched successfully>
2022-04-28T09:23:24,995 [INFO ] W-9001-my_tc_1.0-stdout MODEL_LOG - <All keys matched successfully>
2022-04-28T09:23:51,283 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-04-28T09:23:51,283 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-04-28T09:23:51,372 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.3
TS Home: /opt/conda/lib/python3.6/site-packages
Current directory: /root/tutorial-p-v9wplzjbdxlq/torchserve-demo
Temp directory: /tmp
Number of GPUs: 0
Number of CPUs: 2
Max heap size: 794 M
Python executable: /opt/conda/bin/python3.6
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /root/tutorial-p-v9wplzjbdxlq/torchserve-demo/model_store
Initial Models: my_tc=BERTSeqClassification.mar
Log dir: /root/tutorial-p-v9wplzjbdxlq/torchserve-demo/logs
Metrics dir: /root/tutorial-p-v9wplzjbdxlq/torchserve-demo/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 2
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /root/tutorial-p-v9wplzjbdxlq/torchserve-demo/model_store
Model config: N/A
2022-04-28T09:23:51,372 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.3
TS Home: /opt/conda/lib/python3.6/site-packages
Current directory: /root/tutorial-p-v9wplzjbdxlq/torchserve-demo
Temp directory: /tmp
Number of GPUs: 0
Number of CPUs: 2
Max heap size: 794 M
Python executable: /opt/conda/bin/python3.6
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /root/tutorial-p-v9wplzjbdxlq/torchserve-demo/model_store
Initial Models: my_tc=BERTSeqClassification.mar
Log dir: /root/tutorial-p-v9wplzjbdxlq/torchserve-demo/logs
Metrics dir: /root/tutorial-p-v9wplzjbdxlq/torchserve-demo/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 2
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /root/tutorial-p-v9wplzjbdxlq/torchserve-demo/model_store
Model config: N/A
2022-04-28T09:23:51,380 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-04-28T09:23:51,380 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-04-28T09:23:51,410 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: BERTSeqClassification.mar
2022-04-28T09:23:51,410 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: BERTSeqClassification.mar
2022-04-28T09:23:52,363 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model my_tc
2022-04-28T09:23:52,363 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model my_tc
2022-04-28T09:23:52,364 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model my_tc
2022-04-28T09:23:52,364 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model my_tc
2022-04-28T09:23:52,364 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model my_tc loaded.
2022-04-28T09:23:52,364 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model my_tc loaded.
2022-04-28T09:23:52,364 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: my_tc, count: 2
2022-04-28T09:23:52,364 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: my_tc, count: 2
2022-04-28T09:23:52,391 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-04-28T09:23:52,391 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-04-28T09:23:52,398 [DEBUG] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3.6, /opt/conda/lib/python3.6/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-04-28T09:23:52,399 [DEBUG] W-9001-my_tc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3.6, /opt/conda/lib/python3.6/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001]
2022-04-28T09:23:52,398 [DEBUG] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3.6, /opt/conda/lib/python3.6/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-04-28T09:23:52,399 [DEBUG] W-9001-my_tc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3.6, /opt/conda/lib/python3.6/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001]
2022-04-28T09:23:52,599 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-04-28T09:23:52,599 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-04-28T09:23:52,600 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-04-28T09:23:52,600 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-04-28T09:23:52,606 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-04-28T09:23:52,606 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-04-28T09:23:52,607 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-04-28T09:23:52,607 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-04-28T09:23:52,608 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-04-28T09:23:52,608 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-04-28T09:23:53,170 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-04-28T09:23:53,170 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-04-28T09:23:53,379 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137833
2022-04-28T09:23:53,389 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:26.384445190429688|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137833
2022-04-28T09:23:53,391 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:0.6155548095703125|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137833
2022-04-28T09:23:53,392 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:2.3|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137833
2022-04-28T09:23:53,393 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:1428.19140625|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137833
2022-04-28T09:23:53,394 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:2244.6640625|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137833
2022-04-28T09:23:53,395 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:63.2|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137833
2022-04-28T09:23:53,658 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-04-28T09:23:53,660 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - [PID]6943
2022-04-28T09:23:53,661 [DEBUG] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_tc_1.0 State change null -> WORKER_STARTED
2022-04-28T09:23:53,661 [DEBUG] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_tc_1.0 State change null -> WORKER_STARTED
2022-04-28T09:23:53,665 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-28T09:23:53,665 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - Python runtime: 3.6.13
2022-04-28T09:23:53,676 [INFO ] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-04-28T09:23:53,676 [INFO ] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-04-28T09:23:53,702 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-04-28T09:23:53,706 [INFO ] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1651137833706
2022-04-28T09:23:53,706 [INFO ] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1651137833706
2022-04-28T09:23:53,819 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - model_name: my_tc, batchSize: 1
2022-04-28T09:23:54,097 [INFO ] W-9001-my_tc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2022-04-28T09:23:54,104 [INFO ] W-9001-my_tc_1.0-stdout MODEL_LOG - [PID]6944
2022-04-28T09:23:54,106 [DEBUG] W-9001-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-my_tc_1.0 State change null -> WORKER_STARTED
2022-04-28T09:23:54,106 [DEBUG] W-9001-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-my_tc_1.0 State change null -> WORKER_STARTED
2022-04-28T09:23:54,107 [INFO ] W-9001-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2022-04-28T09:23:54,107 [INFO ] W-9001-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2022-04-28T09:23:54,105 [INFO ] W-9001-my_tc_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-28T09:23:54,110 [INFO ] W-9001-my_tc_1.0-stdout MODEL_LOG - Python runtime: 3.6.13
2022-04-28T09:23:54,132 [INFO ] W-9001-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1651137834131
2022-04-28T09:23:54,132 [INFO ] W-9001-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1651137834131
2022-04-28T09:23:54,132 [INFO ] W-9001-my_tc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2022-04-28T09:23:54,193 [INFO ] W-9001-my_tc_1.0-stdout MODEL_LOG - model_name: my_tc, batchSize: 1
2022-04-28T09:23:54,597 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - Transformers version 4.18.0
2022-04-28T09:23:54,905 [INFO ] W-9001-my_tc_1.0-stdout MODEL_LOG - Transformers version 4.18.0
2022-04-28T09:23:56,854 [WARN ] W-9000-my_tc_1.0-stderr MODEL_LOG - Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias']
2022-04-28T09:23:56,858 [WARN ] W-9000-my_tc_1.0-stderr MODEL_LOG - - This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
2022-04-28T09:23:56,859 [WARN ] W-9000-my_tc_1.0-stderr MODEL_LOG - - This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2022-04-28T09:23:56,859 [WARN ] W-9000-my_tc_1.0-stderr MODEL_LOG - Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']
2022-04-28T09:23:56,859 [WARN ] W-9000-my_tc_1.0-stderr MODEL_LOG - You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2022-04-28T09:23:56,893 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - <All keys matched successfully>
2022-04-28T09:23:57,149 [WARN ] W-9001-my_tc_1.0-stderr MODEL_LOG - Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.bias']
2022-04-28T09:23:57,153 [WARN ] W-9001-my_tc_1.0-stderr MODEL_LOG - - This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
2022-04-28T09:23:57,155 [WARN ] W-9001-my_tc_1.0-stderr MODEL_LOG - - This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2022-04-28T09:23:57,156 [WARN ] W-9001-my_tc_1.0-stderr MODEL_LOG - Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']
2022-04-28T09:23:57,156 [WARN ] W-9001-my_tc_1.0-stderr MODEL_LOG - You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2022-04-28T09:23:57,211 [INFO ] W-9001-my_tc_1.0-stdout MODEL_LOG - <All keys matched successfully>
2022-04-28T09:24:06,186 [INFO ] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 12365
2022-04-28T09:24:06,186 [INFO ] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 12365
2022-04-28T09:24:06,188 [DEBUG] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_tc_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-28T09:24:06,188 [DEBUG] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_tc_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-28T09:24:06,189 [INFO ] W-9000-my_tc_1.0 TS_METRICS - W-9000-my_tc_1.0.ms:13817|#Level:Host|#hostname:Unknown,timestamp:1651137846
2022-04-28T09:24:06,197 [INFO ] W-9000-my_tc_1.0 TS_METRICS - WorkerThreadTime.ms:120|#Level:Host|#hostname:Unknown,timestamp:null
2022-04-28T09:24:06,474 [INFO ] W-9001-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 12285
2022-04-28T09:24:06,474 [INFO ] W-9001-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 12285
2022-04-28T09:24:06,474 [DEBUG] W-9001-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-my_tc_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-28T09:24:06,474 [DEBUG] W-9001-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-my_tc_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-28T09:24:06,474 [INFO ] W-9001-my_tc_1.0 TS_METRICS - W-9001-my_tc_1.0.ms:14096|#Level:Host|#hostname:Unknown,timestamp:1651137846
2022-04-28T09:24:06,475 [INFO ] W-9001-my_tc_1.0 TS_METRICS - WorkerThreadTime.ms:59|#Level:Host|#hostname:Unknown,timestamp:null
2022-04-28T09:24:49,440 [INFO ] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1651137889440
2022-04-28T09:24:49,440 [INFO ] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1651137889440
2022-04-28T09:24:49,452 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - Backend received inference at: 1651137889
2022-04-28T09:24:49,458 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - Received text: '{"text":"Bloomberg has decided to publish a new report on the global economy.", "target":1} '
2022-04-28T09:24:49,453 [WARN ] W-9000-my_tc_1.0-stderr MODEL_LOG - Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
2022-04-28T09:24:49,459 [WARN ] W-9000-my_tc_1.0-stderr MODEL_LOG - /opt/conda/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
2022-04-28T09:24:49,459 [WARN ] W-9000-my_tc_1.0-stderr MODEL_LOG -   FutureWarning,
2022-04-28T09:24:49,703 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - [2022-04-28 09:24:49.702 1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea:6943 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None
2022-04-28T09:24:50,015 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - [2022-04-28 09:24:50.015 1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea:6943 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.
2022-04-28T09:24:50,378 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - This the output size from the Seq classification model torch.Size([1, 2])
2022-04-28T09:24:50,379 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - This the output from the Seq classification model SequenceClassifierOutput(loss=None, logits=tensor([[-0.3778,  0.0593]]), hidden_states=None, attentions=None)
2022-04-28T09:24:50,383 [INFO ] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 941
2022-04-28T09:24:50,383 [INFO ] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 941
2022-04-28T09:24:50,382 [INFO ] W-9000-my_tc_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:926.49|#ModelName:my_tc,Level:Model|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,requestID:601ebfb4-268e-4ac1-a438-2f327b1f9d58,timestamp:1651137890
2022-04-28T09:24:50,384 [INFO ] W-9000-my_tc_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:926.56|#ModelName:my_tc,Level:Model|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,requestID:601ebfb4-268e-4ac1-a438-2f327b1f9d58,timestamp:1651137890
2022-04-28T09:24:50,384 [INFO ] W-9000-my_tc_1.0 ACCESS_LOG - /127.0.0.1:35226 "POST /predictions/my_tc HTTP/1.1" 200 957
2022-04-28T09:24:50,387 [INFO ] W-9000-my_tc_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:Unknown,timestamp:null
2022-04-28T09:24:50,389 [DEBUG] W-9000-my_tc_1.0 org.pytorch.serve.job.Job - Waiting time ns: 181278, Backend time ns: 949764945
2022-04-28T09:24:50,389 [DEBUG] W-9000-my_tc_1.0 org.pytorch.serve.job.Job - Waiting time ns: 181278, Backend time ns: 949764945
2022-04-28T09:24:50,389 [INFO ] W-9000-my_tc_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:Unknown,timestamp:null
2022-04-28T09:24:50,390 [INFO ] W-9000-my_tc_1.0 TS_METRICS - WorkerThreadTime.ms:8|#Level:Host|#hostname:Unknown,timestamp:null
2022-04-28T09:24:53,222 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137893
2022-04-28T09:24:53,223 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:26.384445190429688|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137893
2022-04-28T09:24:53,223 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:0.6155548095703125|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137893
2022-04-28T09:24:53,223 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:2.3|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137893
2022-04-28T09:24:53,223 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:1062.12890625|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137893
2022-04-28T09:24:53,223 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:2610.50390625|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137893
2022-04-28T09:24:53,224 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:72.7|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137893
2022-04-28T09:25:08,642 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-04-28T09:25:08,642 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-04-28T09:25:08,751 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.3
TS Home: /opt/conda/lib/python3.6/site-packages
Current directory: /root/tutorial-p-v9wplzjbdxlq/torchserve-demo
Temp directory: /tmp
Number of GPUs: 0
Number of CPUs: 2
Max heap size: 794 M
Python executable: /opt/conda/bin/python3.6
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /root/tutorial-p-v9wplzjbdxlq/torchserve-demo/model_store
Initial Models: my_tc=BERTSeqClassification.mar
Log dir: /root/tutorial-p-v9wplzjbdxlq/torchserve-demo/logs
Metrics dir: /root/tutorial-p-v9wplzjbdxlq/torchserve-demo/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 2
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /root/tutorial-p-v9wplzjbdxlq/torchserve-demo/model_store
Model config: N/A
2022-04-28T09:25:08,751 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.3
TS Home: /opt/conda/lib/python3.6/site-packages
Current directory: /root/tutorial-p-v9wplzjbdxlq/torchserve-demo
Temp directory: /tmp
Number of GPUs: 0
Number of CPUs: 2
Max heap size: 794 M
Python executable: /opt/conda/bin/python3.6
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /root/tutorial-p-v9wplzjbdxlq/torchserve-demo/model_store
Initial Models: my_tc=BERTSeqClassification.mar
Log dir: /root/tutorial-p-v9wplzjbdxlq/torchserve-demo/logs
Metrics dir: /root/tutorial-p-v9wplzjbdxlq/torchserve-demo/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 2
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /root/tutorial-p-v9wplzjbdxlq/torchserve-demo/model_store
Model config: N/A
2022-04-28T09:25:08,779 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-04-28T09:25:08,779 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-04-28T09:25:08,815 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: BERTSeqClassification.mar
2022-04-28T09:25:08,815 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: BERTSeqClassification.mar
2022-04-28T09:25:10,223 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model my_tc
2022-04-28T09:25:10,223 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model my_tc
2022-04-28T09:25:10,223 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model my_tc
2022-04-28T09:25:10,223 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model my_tc
2022-04-28T09:25:10,223 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model my_tc loaded.
2022-04-28T09:25:10,223 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model my_tc loaded.
2022-04-28T09:25:10,224 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: my_tc, count: 2
2022-04-28T09:25:10,224 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: my_tc, count: 2
2022-04-28T09:25:10,240 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-04-28T09:25:10,240 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-04-28T09:25:10,244 [DEBUG] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3.6, /opt/conda/lib/python3.6/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-04-28T09:25:10,244 [DEBUG] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3.6, /opt/conda/lib/python3.6/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000]
2022-04-28T09:25:10,249 [DEBUG] W-9001-my_tc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3.6, /opt/conda/lib/python3.6/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001]
2022-04-28T09:25:10,249 [DEBUG] W-9001-my_tc_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/opt/conda/bin/python3.6, /opt/conda/lib/python3.6/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001]
2022-04-28T09:25:10,441 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-04-28T09:25:10,441 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-04-28T09:25:10,442 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-04-28T09:25:10,442 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2022-04-28T09:25:10,449 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-04-28T09:25:10,449 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-04-28T09:25:10,450 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-04-28T09:25:10,450 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-04-28T09:25:10,452 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-04-28T09:25:10,452 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-04-28T09:25:10,977 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-04-28T09:25:10,977 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-04-28T09:25:11,121 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137911
2022-04-28T09:25:11,123 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:26.384445190429688|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137911
2022-04-28T09:25:11,124 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:0.6155548095703125|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137911
2022-04-28T09:25:11,124 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:2.3|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137911
2022-04-28T09:25:11,124 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:1399.1875|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137911
2022-04-28T09:25:11,125 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:2275.66796875|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137911
2022-04-28T09:25:11,125 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:64.0|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137911
2022-04-28T09:25:11,575 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2022-04-28T09:25:11,582 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - [PID]7098
2022-04-28T09:25:11,583 [DEBUG] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_tc_1.0 State change null -> WORKER_STARTED
2022-04-28T09:25:11,583 [DEBUG] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_tc_1.0 State change null -> WORKER_STARTED
2022-04-28T09:25:11,582 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-28T09:25:11,583 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - Python runtime: 3.6.13
2022-04-28T09:25:11,591 [INFO ] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-04-28T09:25:11,591 [INFO ] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2022-04-28T09:25:11,607 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2022-04-28T09:25:11,611 [INFO ] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1651137911611
2022-04-28T09:25:11,611 [INFO ] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1651137911611
2022-04-28T09:25:11,676 [INFO ] W-9001-my_tc_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2022-04-28T09:25:11,709 [INFO ] W-9001-my_tc_1.0-stdout MODEL_LOG - [PID]7099
2022-04-28T09:25:11,710 [DEBUG] W-9001-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-my_tc_1.0 State change null -> WORKER_STARTED
2022-04-28T09:25:11,710 [DEBUG] W-9001-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-my_tc_1.0 State change null -> WORKER_STARTED
2022-04-28T09:25:11,710 [INFO ] W-9001-my_tc_1.0-stdout MODEL_LOG - Torch worker started.
2022-04-28T09:25:11,711 [INFO ] W-9001-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2022-04-28T09:25:11,711 [INFO ] W-9001-my_tc_1.0-stdout MODEL_LOG - Python runtime: 3.6.13
2022-04-28T09:25:11,711 [INFO ] W-9001-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2022-04-28T09:25:11,717 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - model_name: my_tc, batchSize: 1
2022-04-28T09:25:11,726 [INFO ] W-9001-my_tc_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2022-04-28T09:25:11,727 [INFO ] W-9001-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1651137911727
2022-04-28T09:25:11,727 [INFO ] W-9001-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1651137911727
2022-04-28T09:25:11,797 [INFO ] W-9001-my_tc_1.0-stdout MODEL_LOG - model_name: my_tc, batchSize: 1
2022-04-28T09:25:12,541 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - Transformers version 4.18.0
2022-04-28T09:25:12,561 [INFO ] W-9001-my_tc_1.0-stdout MODEL_LOG - Transformers version 4.18.0
2022-04-28T09:25:14,890 [WARN ] W-9000-my_tc_1.0-stderr MODEL_LOG - Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias']
2022-04-28T09:25:14,896 [WARN ] W-9000-my_tc_1.0-stderr MODEL_LOG - - This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
2022-04-28T09:25:14,898 [WARN ] W-9000-my_tc_1.0-stderr MODEL_LOG - - This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2022-04-28T09:25:14,899 [WARN ] W-9000-my_tc_1.0-stderr MODEL_LOG - Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']
2022-04-28T09:25:14,900 [WARN ] W-9000-my_tc_1.0-stderr MODEL_LOG - You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2022-04-28T09:25:14,917 [WARN ] W-9001-my_tc_1.0-stderr MODEL_LOG - Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
2022-04-28T09:25:14,921 [WARN ] W-9001-my_tc_1.0-stderr MODEL_LOG - - This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
2022-04-28T09:25:14,924 [WARN ] W-9001-my_tc_1.0-stderr MODEL_LOG - - This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2022-04-28T09:25:14,926 [WARN ] W-9001-my_tc_1.0-stderr MODEL_LOG - Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']
2022-04-28T09:25:14,928 [WARN ] W-9001-my_tc_1.0-stderr MODEL_LOG - You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2022-04-28T09:25:14,962 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - <All keys matched successfully>
2022-04-28T09:25:14,997 [INFO ] W-9001-my_tc_1.0-stdout MODEL_LOG - <All keys matched successfully>
2022-04-28T09:25:24,085 [INFO ] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 12357
2022-04-28T09:25:24,085 [INFO ] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 12357
2022-04-28T09:25:24,088 [DEBUG] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_tc_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-28T09:25:24,088 [DEBUG] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-my_tc_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-28T09:25:24,088 [INFO ] W-9000-my_tc_1.0 TS_METRICS - W-9000-my_tc_1.0.ms:13855|#Level:Host|#hostname:Unknown,timestamp:1651137924
2022-04-28T09:25:24,091 [INFO ] W-9000-my_tc_1.0 TS_METRICS - WorkerThreadTime.ms:123|#Level:Host|#hostname:Unknown,timestamp:null
2022-04-28T09:25:24,091 [INFO ] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1651137924091
2022-04-28T09:25:24,091 [INFO ] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1651137924091
2022-04-28T09:25:24,099 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - Backend received inference at: 1651137924
2022-04-28T09:25:24,100 [WARN ] W-9000-my_tc_1.0-stderr MODEL_LOG - Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
2022-04-28T09:25:24,104 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - Received text: '{"text":"Bloomberg has decided to publish a new report on the global economy.", "target":1} '
2022-04-28T09:25:24,106 [WARN ] W-9000-my_tc_1.0-stderr MODEL_LOG - /opt/conda/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
2022-04-28T09:25:24,107 [WARN ] W-9000-my_tc_1.0-stderr MODEL_LOG -   FutureWarning,
2022-04-28T09:25:24,202 [INFO ] W-9001-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 12400
2022-04-28T09:25:24,202 [INFO ] W-9001-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 12400
2022-04-28T09:25:24,203 [DEBUG] W-9001-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-my_tc_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-28T09:25:24,203 [DEBUG] W-9001-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-my_tc_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2022-04-28T09:25:24,204 [INFO ] W-9001-my_tc_1.0 TS_METRICS - W-9001-my_tc_1.0.ms:13967|#Level:Host|#hostname:Unknown,timestamp:1651137924
2022-04-28T09:25:24,205 [INFO ] W-9001-my_tc_1.0 TS_METRICS - WorkerThreadTime.ms:77|#Level:Host|#hostname:Unknown,timestamp:null
2022-04-28T09:25:24,366 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - [2022-04-28 09:25:24.366 1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea:7098 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None
2022-04-28T09:25:24,604 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - [2022-04-28 09:25:24.604 1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea:7098 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.
2022-04-28T09:25:25,004 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - This the output size from the Seq classification model torch.Size([1, 2])
2022-04-28T09:25:25,006 [INFO ] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 908
2022-04-28T09:25:25,005 [INFO ] W-9000-my_tc_1.0-stdout MODEL_LOG - This the output from the Seq classification model SequenceClassifierOutput(loss=None, logits=tensor([[-1.1051,  0.6561]]), hidden_states=None, attentions=None)
2022-04-28T09:25:25,006 [INFO ] W-9000-my_tc_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 908
2022-04-28T09:25:25,008 [INFO ] W-9000-my_tc_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:905.26|#ModelName:my_tc,Level:Model|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,requestID:21bba63c-1f51-4b14-a559-75063fbc6994,timestamp:1651137925
2022-04-28T09:25:25,011 [INFO ] W-9000-my_tc_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:905.34|#ModelName:my_tc,Level:Model|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,requestID:21bba63c-1f51-4b14-a559-75063fbc6994,timestamp:1651137925
2022-04-28T09:25:25,010 [INFO ] W-9000-my_tc_1.0 ACCESS_LOG - /127.0.0.1:35466 "POST /predictions/my_tc HTTP/1.1" 200 5232
2022-04-28T09:25:25,011 [INFO ] W-9000-my_tc_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:Unknown,timestamp:null
2022-04-28T09:25:25,012 [DEBUG] W-9000-my_tc_1.0 org.pytorch.serve.job.Job - Waiting time ns: 4300030505, Backend time ns: 920483016
2022-04-28T09:25:25,012 [DEBUG] W-9000-my_tc_1.0 org.pytorch.serve.job.Job - Waiting time ns: 4300030505, Backend time ns: 920483016
2022-04-28T09:25:25,012 [INFO ] W-9000-my_tc_1.0 TS_METRICS - QueueTime.ms:4300|#Level:Host|#hostname:Unknown,timestamp:null
2022-04-28T09:25:25,012 [INFO ] W-9000-my_tc_1.0 TS_METRICS - WorkerThreadTime.ms:13|#Level:Host|#hostname:Unknown,timestamp:null
2022-04-28T09:26:11,013 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137971
2022-04-28T09:26:11,017 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:26.384445190429688|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137971
2022-04-28T09:26:11,017 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:0.6155548095703125|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137971
2022-04-28T09:26:11,018 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:2.3|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137971
2022-04-28T09:26:11,018 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:1072.2421875|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137971
2022-04-28T09:26:11,018 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:2601.80859375|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137971
2022-04-28T09:26:11,018 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:72.4|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651137971
2022-04-28T09:27:11,057 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138031
2022-04-28T09:27:11,062 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:26.384445190429688|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138031
2022-04-28T09:27:11,063 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:0.6155548095703125|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138031
2022-04-28T09:27:11,063 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:2.3|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138031
2022-04-28T09:27:11,064 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:1071.6875|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138031
2022-04-28T09:27:11,064 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:2602.35546875|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138031
2022-04-28T09:27:11,065 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:72.4|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138031
2022-04-28T09:28:11,029 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138091
2022-04-28T09:28:11,033 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:26.384445190429688|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138091
2022-04-28T09:28:11,035 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:0.6155548095703125|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138091
2022-04-28T09:28:11,041 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:2.3|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138091
2022-04-28T09:28:11,042 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:1071.96875|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138091
2022-04-28T09:28:11,043 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:2602.1953125|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138091
2022-04-28T09:28:11,043 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:72.4|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138091
2022-04-28T09:29:11,004 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138151
2022-04-28T09:29:11,007 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:26.383960723876953|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138151
2022-04-28T09:29:11,007 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:0.6160392761230469|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138151
2022-04-28T09:29:11,007 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:2.3|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138151
2022-04-28T09:29:11,008 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:1069.953125|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138151
2022-04-28T09:29:11,008 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:2605.265625|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138151
2022-04-28T09:29:11,008 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:72.5|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138151
2022-04-28T09:30:11,063 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138211
2022-04-28T09:30:11,067 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:26.383960723876953|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138211
2022-04-28T09:30:11,068 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:0.6160392761230469|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138211
2022-04-28T09:30:11,071 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:2.3|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138211
2022-04-28T09:30:11,073 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:1066.734375|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138211
2022-04-28T09:30:11,074 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:2608.71484375|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138211
2022-04-28T09:30:11,075 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:72.5|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138211
2022-04-28T09:31:11,013 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138271
2022-04-28T09:31:11,017 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:26.383960723876953|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138271
2022-04-28T09:31:11,017 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:0.6160392761230469|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138271
2022-04-28T09:31:11,017 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:2.3|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138271
2022-04-28T09:31:11,017 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:1071.07421875|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138271
2022-04-28T09:31:11,018 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:2604.39453125|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138271
2022-04-28T09:31:11,018 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:72.4|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138271
2022-04-28T09:32:11,121 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138331
2022-04-28T09:32:11,127 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:26.383960723876953|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138331
2022-04-28T09:32:11,127 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:0.6160392761230469|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138331
2022-04-28T09:32:11,127 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:2.3|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138331
2022-04-28T09:32:11,127 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:1071.359375|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138331
2022-04-28T09:32:11,128 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:2604.3046875|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138331
2022-04-28T09:32:11,128 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:72.4|#Level:Host|#hostname:1-8-1-cpu-py36-ml-t3-medium-1290f598ee5787c8b7e772204dea,timestamp:1651138331
2022-04-28T09:33:07,812 [WARN ] W-9001-my_tc_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-04-28T09:33:07,813 [WARN ] W-9000-my_tc_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2022-04-28T09:33:07,819 [WARN ] W-9001-my_tc_1.0-stderr MODEL_LOG -   File "/opt/conda/lib/python3.6/site-packages/ts/model_service_worker.py", line 189, in <module>
2022-04-28T09:33:07,820 [WARN ] W-9000-my_tc_1.0-stderr MODEL_LOG -   File "/opt/conda/lib/python3.6/site-packages/ts/model_service_worker.py", line 189, in <module>
2022-04-28T09:33:07,820 [WARN ] W-9001-my_tc_1.0-stderr MODEL_LOG -     worker.run_server()
2022-04-28T09:33:07,820 [WARN ] W-9000-my_tc_1.0-stderr MODEL_LOG -     worker.run_server()
2022-04-28T09:33:07,820 [WARN ] W-9001-my_tc_1.0-stderr MODEL_LOG -   File "/opt/conda/lib/python3.6/site-packages/ts/model_service_worker.py", line 161, in run_server
2022-04-28T09:33:07,820 [WARN ] W-9000-my_tc_1.0-stderr MODEL_LOG -   File "/opt/conda/lib/python3.6/site-packages/ts/model_service_worker.py", line 161, in run_server
2022-04-28T09:33:07,820 [WARN ] W-9001-my_tc_1.0-stderr MODEL_LOG -     self.handle_connection(cl_socket)
2022-04-28T09:33:07,820 [WARN ] W-9000-my_tc_1.0-stderr MODEL_LOG -     self.handle_connection(cl_socket)
2022-04-28T09:33:07,820 [WARN ] W-9001-my_tc_1.0-stderr MODEL_LOG -   File "/opt/conda/lib/python3.6/site-packages/ts/model_service_worker.py", line 116, in handle_connection
2022-04-28T09:33:07,822 [WARN ] W-9001-my_tc_1.0-stderr MODEL_LOG -     cmd, msg = retrieve_msg(cl_socket)
2022-04-28T09:33:07,822 [WARN ] W-9000-my_tc_1.0-stderr MODEL_LOG -   File "/opt/conda/lib/python3.6/site-packages/ts/model_service_worker.py", line 116, in handle_connection
2022-04-28T09:33:07,823 [WARN ] W-9000-my_tc_1.0-stderr MODEL_LOG -     cmd, msg = retrieve_msg(cl_socket)
2022-04-28T09:33:07,823 [WARN ] W-9001-my_tc_1.0-stderr MODEL_LOG -   File "/opt/conda/lib/python3.6/site-packages/ts/protocol/otf_message_handler.py", line 32, in retrieve_msg
2022-04-28T09:33:07,823 [WARN ] W-9001-my_tc_1.0-stderr MODEL_LOG -     cmd = _retrieve_buffer(conn, 1)
2022-04-28T09:33:07,824 [WARN ] W-9001-my_tc_1.0-stderr MODEL_LOG -   File "/opt/conda/lib/python3.6/site-packages/ts/protocol/otf_message_handler.py", line 164, in _retrieve_buffer
2022-04-28T09:33:07,825 [WARN ] W-9001-my_tc_1.0-stderr MODEL_LOG -     pkt = conn.recv(length)
2022-04-28T09:33:07,826 [WARN ] W-9001-my_tc_1.0-stderr MODEL_LOG - KeyboardInterrupt
2022-04-28T09:33:07,823 [WARN ] W-9000-my_tc_1.0-stderr MODEL_LOG -   File "/opt/conda/lib/python3.6/site-packages/ts/protocol/otf_message_handler.py", line 32, in retrieve_msg
2022-04-28T09:33:07,827 [WARN ] W-9000-my_tc_1.0-stderr MODEL_LOG -     cmd = _retrieve_buffer(conn, 1)
2022-04-28T09:33:07,827 [WARN ] W-9000-my_tc_1.0-stderr MODEL_LOG -   File "/opt/conda/lib/python3.6/site-packages/ts/protocol/otf_message_handler.py", line 164, in _retrieve_buffer
2022-04-28T09:33:07,828 [WARN ] W-9000-my_tc_1.0-stderr MODEL_LOG -     pkt = conn.recv(length)
2022-04-28T09:33:07,828 [WARN ] W-9000-my_tc_1.0-stderr MODEL_LOG - KeyboardInterrupt
