{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install torchserve torch-model-archiver torch-workflow-archiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile -a inference_handler.py\n",
    "from abc import ABC\n",
    "import logging\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import os\n",
    "import json\n",
    "from ts.torch_handler.base_handler import BaseHandler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile -a inference_handler.py\n",
    "logging.basicConfig(\n",
    "    level={\"ERROR\": logging.ERROR, \"INFO\": logging.INFO, \"DEBUG\": logging.DEBUG}[\"INFO\"],\n",
    "    filename=\"./torserve.log\",\n",
    "    filemode='a',\n",
    "    format=\n",
    "    '%(asctime)s - %(pathname)s[line:%(lineno)d] - %(levelname)s: %(message)s'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile -a inference_handler.py\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"Transformers version %s\",transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup config file\n",
    "```json\n",
    "{\n",
    " \"model_name\":\"albert-base-v2\",\n",
    " \"mode\":\"sequence_classification\",\n",
    " \"do_lower_case\":true,\n",
    " \"num_labels\":\"2\",\n",
    " \"max_length\":\"150\"\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_config_path = os.path.join(\"setup_config.json\")\n",
    "if os.path.isfile(setup_config_path):\n",
    "    with open(setup_config_path) as setup_config_file:\n",
    "        setup_config = json.load(setup_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'albert-base-v2',\n",
       " 'mode': 'sequence_classification',\n",
       " 'do_lower_case': True,\n",
       " 'num_labels': '2',\n",
       " 'max_length': '150'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile -a inference_handler.py\n",
    "class TransformersSeqClassifierHandler(BaseHandler, ABC):\n",
    "    \"\"\"\n",
    "    Transformers handler class for sequence, token classification and question answering.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TransformersSeqClassifierHandler, self).__init__()\n",
    "        self.initialized = False\n",
    "\n",
    "    def initialize(self, ctx):\n",
    "        \"\"\"In this initialize function, the BERT model is loaded and\n",
    "        the Layer Integrated Gradients Algorithm for Captum Explanations\n",
    "        is initialized here.\n",
    "        Args:\n",
    "            ctx (context): It is a JSON Object containing information\n",
    "            pertaining to the model artefacts parameters.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.manifest = ctx.manifest\n",
    "        properties = ctx.system_properties\n",
    "        model_dir = properties.get(\"model_dir\")\n",
    "        \n",
    "        # read configs for the mode, model_name, etc. from setup_config.json\n",
    "        setup_config_path = os.path.join(model_dir, \"setup_config.json\")\n",
    "        if os.path.isfile(setup_config_path):\n",
    "            with open(setup_config_path) as setup_config_file:\n",
    "                self.setup_config = json.load(setup_config_file)\n",
    "        else:\n",
    "            logger.warning(\"Missing the setup_config.json file.\")\n",
    "        \n",
    "        #Load Model\n",
    "        serialized_file = self.manifest[\"model\"][\"serializedFile\"]\n",
    "        model_pt_path = os.path.join(model_dir, serialized_file)\n",
    "        self.device = \"cuda\"if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.setup_config['model_name'], num_labels=int(self.setup_config['num_labels']))\n",
    "        load_model_info = self.model.load_state_dict(torch.load(model_pt_path))\n",
    "        logger.info(load_model_info)\n",
    "        self.model.eval()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.setup_config['model_name'])\n",
    "        self.initialized = True \n",
    "            \n",
    "\n",
    "    def preprocess(self, requests):\n",
    "        \"\"\"Basic text preprocessing, based on the user's chocie of application mode.\n",
    "        Args:\n",
    "            requests (str): The Input data in the form of text is passed on to the preprocess\n",
    "            function.\n",
    "        Returns:\n",
    "            list : The preprocess function returns a list of Tensor for the size of the word tokens.\n",
    "        \"\"\"\n",
    "        input_ids_batch = None\n",
    "        attention_mask_batch = None\n",
    "        for idx, data in enumerate(requests):\n",
    "            input_text = data.get(\"data\")\n",
    "            if input_text is None:\n",
    "                input_text = data.get(\"body\")\n",
    "            if isinstance(input_text, (bytes, bytearray)):\n",
    "                input_text = input_text.decode('utf-8')\n",
    "            logger.info(\"Received text: '%s'\", input_text)\n",
    "            inputs = self.tokenizer.encode_plus(input_text, max_length=int(self.setup_config['max_length']),\n",
    "                                                    pad_to_max_length=True, add_special_tokens=True, return_tensors='pt')\n",
    "\n",
    "            input_ids = inputs[\"input_ids\"].to(self.device)\n",
    "            attention_mask = inputs[\"attention_mask\"].to(self.device)\n",
    "\n",
    "            if input_ids.shape is not None:\n",
    "                if input_ids_batch is None:\n",
    "                    input_ids_batch = input_ids\n",
    "                    attention_mask_batch = attention_mask\n",
    "                else:\n",
    "                    input_ids_batch = torch.cat((input_ids_batch, input_ids), 0)\n",
    "                    attention_mask_batch = torch.cat((attention_mask_batch, attention_mask), 0)\n",
    "                    \n",
    "        return (input_ids_batch, attention_mask_batch)\n",
    "\n",
    "    def inference(self, input_batch):\n",
    "        \"\"\"Predict the class (or classes) of the received text using the\n",
    "        serialized transformers checkpoint.\n",
    "        Args:\n",
    "            input_batch (list): List of Text Tensors from the pre-process function is passed here\n",
    "        Returns:\n",
    "            list : It returns a list of the predicted value for the input text\n",
    "        \"\"\"\n",
    "        input_ids_batch, attention_mask_batch = input_batch\n",
    "        inferences = []\n",
    "        # Handling inference for sequence_classification.\n",
    "        if self.setup_config[\"mode\"] == \"sequence_classification\":\n",
    "            with torch.no_grad():\n",
    "                predictions = self.model(input_ids_batch, attention_mask_batch)\n",
    "            print(\"This the output size from the Seq classification model\", predictions[0].size())\n",
    "            print(\"This the output from the Seq classification model\", predictions)\n",
    "\n",
    "            num_rows, num_cols = predictions[0].shape\n",
    "            for i in range(num_rows):\n",
    "                out = predictions[0][i].unsqueeze(0)\n",
    "                y_hat = out.argmax(1).item()\n",
    "                prob = torch.sigmoid(out[0])[y_hat].item()\n",
    "                predicted_idx = str(y_hat)\n",
    "                inferences.append({\"pred\":predicted_idx,\"prob\":prob})\n",
    "        return inferences\n",
    "\n",
    "    def postprocess(self, inference_output):\n",
    "        \"\"\"Post Process Function converts the predicted response into Torchserve readable format.\n",
    "        Args:\n",
    "            inference_output (list): It contains the predicted response of the input text.\n",
    "        Returns:\n",
    "            (list): Returns a list of the Predictions and Explanations.\n",
    "        \"\"\"\n",
    "\n",
    "        idex_dict = {\"0\":\"Unaccetable\",\"1\":\"Accetable\"}\n",
    "\n",
    "        inference_output = [idex_dict[inf]for inf in inference_output]\n",
    "\n",
    "        return inference_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class self:\n",
    "    temp = \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_config_path = os.path.join(model_dir, \"setup_config.json\")\n",
    "if os.path.isfile(setup_config_path):\n",
    "    with open(setup_config_path) as setup_config_file:\n",
    "        self.setup_config = json.load(setup_config_file)\n",
    "else:\n",
    "    logger.warning(\"Missing the setup_config.json file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Load Model\n",
    "# serialized_file = self.manifest[\"model\"][\"serializedFile\"]\n",
    "serialized_file = \"models_saved/pytorch_model.pt\"\n",
    "model_pt_path = os.path.join(model_dir, serialized_file)\n",
    "self.device = \"cuda\"if torch.cuda.is_available() else \"cpu\"\n",
    "self.model = AutoModelForSequenceClassification.from_pretrained(self.setup_config['model_name'], num_labels=int(self.setup_config['num_labels']))\n",
    "load_model_info = self.model.load_state_dict(torch.load(model_pt_path))\n",
    "logger.info(load_model_info)\n",
    "self.model.eval()\n",
    "self.tokenizer = AutoTokenizer.from_pretrained(self.setup_config['model_name'])\n",
    "self.initialized = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models_saved/pytorch_model.pt'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pt_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests = [{\"data\":\"Bloomberg has decided to publish a new report on the global economy. \"},\n",
    "{\"data\":\"Distance learning has become a norm so far for schooling.\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(\"hood\", (bytes, bytearray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/opt/conda/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "input_ids_batch = None\n",
    "attention_mask_batch = None\n",
    "for idx, data in enumerate(requests):\n",
    "    input_text = data.get(\"data\")\n",
    "    if input_text is None:\n",
    "        input_text = data.get(\"body\")\n",
    "    if isinstance(input_text, (bytes, bytearray)):\n",
    "        input_text = input_text.decode('utf-8')\n",
    "    logger.info(\"Received text: '%s'\", input_text)\n",
    "    inputs = self.tokenizer.encode_plus(input_text, max_length=int(self.setup_config['max_length']),\n",
    "                                            pad_to_max_length=True, add_special_tokens=True, return_tensors='pt')\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"].to(self.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(self.device)\n",
    "\n",
    "    if input_ids.shape is not None:\n",
    "        if input_ids_batch is None:\n",
    "            input_ids_batch = input_ids\n",
    "            attention_mask_batch = attention_mask\n",
    "        else:\n",
    "            input_ids_batch = torch.cat((input_ids_batch, input_ids), 0)\n",
    "            attention_mask_batch = torch.cat((attention_mask_batch, attention_mask), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (input_ids_batch, attention_mask_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = (input_ids_batch, attention_mask_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This the output size from the Seq classification model torch.Size([2, 2])\n",
      "This the output from the Seq classification model SequenceClassifierOutput(loss=None, logits=tensor([[-1.5778,  1.0205],\n",
      "        [-1.1940,  0.9610]]), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "input_ids_batch, attention_mask_batch = input_batch\n",
    "inferences = []\n",
    "# Handling inference for sequence_classification.\n",
    "if self.setup_config[\"mode\"] == \"sequence_classification\":\n",
    "    with torch.no_grad():\n",
    "        predictions = self.model(input_ids_batch, attention_mask_batch)\n",
    "    print(\"This the output size from the Seq classification model\", predictions[0].size())\n",
    "    print(\"This the output from the Seq classification model\", predictions)\n",
    "\n",
    "    num_rows, num_cols = predictions[0].shape\n",
    "    for i in range(num_rows):\n",
    "        out = predictions[0][i].unsqueeze(0)\n",
    "        y_hat = out.argmax(1).item()\n",
    "        prob = torch.sigmoid(out[0])[y_hat].item()\n",
    "        predicted_idx = str(y_hat)\n",
    "        inferences.append({\"pred\":predicted_idx,\"prob\":prob})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'pred': '1', 'prob': 0.7350611686706543},\n",
       " {'pred': '1', 'prob': 0.7233153581619263}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) PostProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_input = inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "idex_dict = {\"0\":\"Unaccetable\",\"1\":\"Accetable\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_output = []\n",
    "for res in inference_input:\n",
    "    sample = {}\n",
    "    sample['pred'] = idex_dict[res[\"pred\"]]\n",
    "    sample['prob'] = res[\"prob\"]\n",
    "    inference_output.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'pred': 'Accetable', 'prob': 0.7350611686706543},\n",
       " {'pred': 'Accetable', 'prob': 0.7233153581619263}]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Write Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inference_handler.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference_handler.py\n",
    "from abc import ABC\n",
    "import logging\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import os\n",
    "import json\n",
    "from ts.torch_handler.base_handler import BaseHandler\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level={\"ERROR\": logging.ERROR, \"INFO\": logging.INFO, \"DEBUG\": logging.DEBUG}[\"INFO\"],\n",
    "    filename=\"./torserve.log\",\n",
    "    filemode='a',\n",
    "    format=\n",
    "    '%(asctime)s - %(pathname)s[line:%(lineno)d] - %(levelname)s: %(message)s'\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"Transformers version %s\",transformers.__version__)\n",
    "\n",
    "class TransformersSeqClassifierHandler(BaseHandler, ABC):\n",
    "    \"\"\"\n",
    "    Transformers handler class for sequence, token classification and question answering.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TransformersSeqClassifierHandler, self).__init__()\n",
    "        self.initialized = False\n",
    "\n",
    "    def initialize(self, ctx):\n",
    "        \"\"\"In this initialize function, the BERT model is loaded and\n",
    "        the Layer Integrated Gradients Algorithm for Captum Explanations\n",
    "        is initialized here.\n",
    "        Args:\n",
    "            ctx (context): It is a JSON Object containing information\n",
    "            pertaining to the model artefacts parameters.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.manifest = ctx.manifest\n",
    "        properties = ctx.system_properties\n",
    "        model_dir = properties.get(\"model_dir\")\n",
    "        \n",
    "        # read configs for the mode, model_name, etc. from setup_config.json\n",
    "        setup_config_path = os.path.join(model_dir, \"setup_config.json\")\n",
    "        if os.path.isfile(setup_config_path):\n",
    "            with open(setup_config_path) as setup_config_file:\n",
    "                self.setup_config = json.load(setup_config_file)\n",
    "        else:\n",
    "            logger.warning(\"Missing the setup_config.json file.\")\n",
    "        \n",
    "        #Load Model\n",
    "        serialized_file = self.manifest[\"model\"][\"serializedFile\"]\n",
    "        model_pt_path = os.path.join(model_dir, serialized_file)\n",
    "        self.device = \"cuda\"if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.setup_config['model_name'], num_labels=int(self.setup_config['num_labels']))\n",
    "        load_model_info = self.model.load_state_dict(torch.load(model_pt_path))\n",
    "        logger.info(load_model_info)\n",
    "        self.model.eval()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.setup_config['model_name'])\n",
    "        self.initialized = True \n",
    "            \n",
    "\n",
    "    def preprocess(self, requests):\n",
    "        \"\"\"Basic text preprocessing, based on the user's chocie of application mode.\n",
    "        Args:\n",
    "            requests (str): The Input data in the form of text is passed on to the preprocess\n",
    "            function.\n",
    "        Returns:\n",
    "            list : The preprocess function returns a list of Tensor for the size of the word tokens.\n",
    "        \"\"\"\n",
    "        input_ids_batch = None\n",
    "        attention_mask_batch = None\n",
    "        for idx, data in enumerate(requests):\n",
    "            input_text = data.get(\"data\")\n",
    "            if input_text is None:\n",
    "                input_text = data.get(\"body\")\n",
    "            if isinstance(input_text, (bytes, bytearray)):\n",
    "                input_text = input_text.decode('utf-8')\n",
    "            logger.info(\"Received text: '%s'\", input_text)\n",
    "            inputs = self.tokenizer.encode_plus(input_text, max_length=int(self.setup_config['max_length']),\n",
    "                                                    pad_to_max_length=True, add_special_tokens=True, return_tensors='pt')\n",
    "\n",
    "            input_ids = inputs[\"input_ids\"].to(self.device)\n",
    "            attention_mask = inputs[\"attention_mask\"].to(self.device)\n",
    "\n",
    "            if input_ids.shape is not None:\n",
    "                if input_ids_batch is None:\n",
    "                    input_ids_batch = input_ids\n",
    "                    attention_mask_batch = attention_mask\n",
    "                else:\n",
    "                    input_ids_batch = torch.cat((input_ids_batch, input_ids), 0)\n",
    "                    attention_mask_batch = torch.cat((attention_mask_batch, attention_mask), 0)\n",
    "                    \n",
    "        return (input_ids_batch, attention_mask_batch)\n",
    "\n",
    "    def inference(self, input_batch):\n",
    "        \"\"\"Predict the class (or classes) of the received text using the\n",
    "        serialized transformers checkpoint.\n",
    "        Args:\n",
    "            input_batch (list): List of Text Tensors from the pre-process function is passed here\n",
    "        Returns:\n",
    "            list : It returns a list of the predicted value for the input text\n",
    "        \"\"\"\n",
    "        input_ids_batch, attention_mask_batch = input_batch\n",
    "        inferences = []\n",
    "        # Handling inference for sequence_classification.\n",
    "        if self.setup_config[\"mode\"] == \"sequence_classification\":\n",
    "            with torch.no_grad():\n",
    "                predictions = self.model(input_ids_batch, attention_mask_batch)\n",
    "            print(\"This the output size from the Seq classification model\", predictions[0].size())\n",
    "            print(\"This the output from the Seq classification model\", predictions)\n",
    "\n",
    "            num_rows, num_cols = predictions[0].shape\n",
    "            for i in range(num_rows):\n",
    "                out = predictions[0][i].unsqueeze(0)\n",
    "                y_hat = out.argmax(1).item()\n",
    "                prob = torch.sigmoid(out[0])[y_hat].item()\n",
    "                predicted_idx = str(y_hat)\n",
    "                inferences.append({\"pred\":predicted_idx,\"prob\":prob})\n",
    "        return inferences\n",
    "\n",
    "    def postprocess(self, inference_input):\n",
    "        \"\"\"Post Process Function converts the predicted response into Torchserve readable format.\n",
    "        Args:\n",
    "            inference_output (list): It contains the predicted response of the input text.\n",
    "        Returns:\n",
    "            (list): Returns a list of the Predictions and Explanations.\n",
    "        \"\"\"\n",
    "\n",
    "        idex_dict = {\"0\":\"Unaccetable\",\"1\":\"Accetable\"}\n",
    "        \n",
    "        inference_output = []\n",
    "        \n",
    "        for res in inference_input:\n",
    "            sample = {}\n",
    "            sample['pred'] = idex_dict[res[\"pred\"]]\n",
    "            sample['prob'] = res[\"prob\"]\n",
    "            inference_output.append(sample)\n",
    "\n",
    "        return inference_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.8 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-1:492261229750:image/1.8.1-cpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
